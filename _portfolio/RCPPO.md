---
title: "Deep-Learning based Tumor Segmentation in Multiparametric MR"
excerpt: " Developed a end-to-end pipeline for automatic tumor segmentation and quantitative analysis for preclinical multiparametric Triple Negative Breast Cancer (TNBC) PDX MR images. The pipeline consists of a novel deep-learning architecture called **Dense Recurrent Residual U-Net (DR2U-Net)** for automatic tumor segmentation. Further we extracted radiomics features from the segmented maps to validate the robustness of segmentation boundaries and estabilished the reporducibility of the framework. Currently our algorithm is deployed in the [PIXI](https://pixi.org/https://pixi.org/) platform to be tested on multi-institutional dataset to estabilish the generalizibility of the model. <br/><img src='/images/dl-seg.JPG'>"
collection: portfolio
---
### Project Description
Preclinical MR imaging is a critical component in the co-clinical research pipeline, both in academia and in industry, to validate imaging biomarkers for detection of diseases and assessing therapeutic efficacy. To that end, T1- and T2- weighted MR images are routinely used to extract morphological and pathological information from tumor lesions. In this context, accurate localization and delineation of tumor boundaries are vital for assessing treatment responseIn this context, accurate localization and delineation of tumor boundaries. Manual segmentation by experts, however, is time and labor intensive and suffers from inter- and intra-observer variability with limited reproducibility. In order to address this challenge our aim was to develop novel deep-learning based architecture DR2U-Net specially optimized to the task of automatically segmenting tumors from multiparametric MR images to alleviate manual effort and to circumvent observer variability in tumor delineation. 


In this project I implemented the [Reward Constrained Policy Optimization Paper](https://openreview.net/pdf?id=SkfrvsA9FX) by Tessler et al. into stable-baselines3 implementation of PPO. Additionally, I reproduced the original results by tracking my experiments using weights and biases. The code for this project can be found [here](https://github.com/sudo-Boris/stable-baselines3). I also wrote an article elaborating on the theory of RCPO and my results and submitted it to the ICLR Blogposts Track! You can fin the article [here](https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/)
